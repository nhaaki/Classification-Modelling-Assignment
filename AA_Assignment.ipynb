{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import csv\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  2225 non-null   object\n",
      " 1   text      2225 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.9+ KB\n"
     ]
    }
   ],
   "source": [
    "dat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2225</td>\n",
       "      <td>2225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>2126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>sport</td>\n",
       "      <td>stars pay tribute to actor davis hollywood sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>511</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                               text\n",
       "count      2225                                               2225\n",
       "unique        5                                               2126\n",
       "top       sport  stars pay tribute to actor davis hollywood sta...\n",
       "freq        511                                                  2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cleanse the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  tv future in the hands of viewers with home th...  \n",
       "1  worldcom boss  left books alone  former worldc...  \n",
       "2  tigers wary of farrell  gamble  leicester say ...  \n",
       "3  yeading face newcastle in fa cup premiership s...  \n",
       "4  ocean s twelve raids box office ocean s twelve...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, remove all punctuations.\n",
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "dat['text_clean'] = dat['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, in, the, hands, of, viewers, with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>[tigers, wary, of, farrell, gamble, leicester,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, in, fa, cup, premie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, s, twelve, raids, box, office, ocean, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                      text_tokenized  \n",
       "0  [tv, future, in, the, hands, of, viewers, with...  \n",
       "1  [worldcom, boss, left, books, alone, former, w...  \n",
       "2  [tigers, wary, of, farrell, gamble, leicester,...  \n",
       "3  [yeading, face, newcastle, in, fa, cup, premie...  \n",
       "4  [ocean, s, twelve, raids, box, office, ocean, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, tokenization\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "dat['text_tokenized'] = dat['text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, in, the, hands, of, viewers, with...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>[worldcom, boss, left, books, former, worldcom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>[tigers, wary, of, farrell, gamble, leicester,...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, in, fa, cup, premie...</td>\n",
       "      <td>[yeading, newcastle, fa, cup, premiership, new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, s, twelve, raids, box, office, ocean, ...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [tv, future, in, the, hands, of, viewers, with...   \n",
       "1  [worldcom, boss, left, books, alone, former, w...   \n",
       "2  [tigers, wary, of, farrell, gamble, leicester,...   \n",
       "3  [yeading, face, newcastle, in, fa, cup, premie...   \n",
       "4  [ocean, s, twelve, raids, box, office, ocean, ...   \n",
       "\n",
       "                                         text_nostop  \n",
       "0  [tv, future, hands, viewers, home, theatre, sy...  \n",
       "1  [worldcom, boss, left, books, former, worldcom...  \n",
       "2  [tigers, wary, farrell, gamble, leicester, rus...  \n",
       "3  [yeading, newcastle, fa, cup, premiership, new...  \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"stopwords.txt\")\n",
    "\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "dat['text_nostop'] = dat['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nostop</th>\n",
       "      <th>text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, in, the, hands, of, viewers, with...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>[worldcom, boss, left, books, former, worldcom...</td>\n",
       "      <td>worldcom bos left book former worldcom bos ber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>[tigers, wary, of, farrell, gamble, leicester,...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, rus...</td>\n",
       "      <td>tiger wary farrell gamble leicester rushed bid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, in, fa, cup, premie...</td>\n",
       "      <td>[yeading, newcastle, fa, cup, premiership, new...</td>\n",
       "      <td>yeading newcastle fa cup premiership newcastle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, s, twelve, raids, box, office, ocean, ...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [tv, future, in, the, hands, of, viewers, with...   \n",
       "1  [worldcom, boss, left, books, alone, former, w...   \n",
       "2  [tigers, wary, of, farrell, gamble, leicester,...   \n",
       "3  [yeading, face, newcastle, in, fa, cup, premie...   \n",
       "4  [ocean, s, twelve, raids, box, office, ocean, ...   \n",
       "\n",
       "                                         text_nostop  \\\n",
       "0  [tv, future, hands, viewers, home, theatre, sy...   \n",
       "1  [worldcom, boss, left, books, former, worldcom...   \n",
       "2  [tigers, wary, farrell, gamble, leicester, rus...   \n",
       "3  [yeading, newcastle, fa, cup, premiership, new...   \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...   \n",
       "\n",
       "                                     text_lemmatized  \n",
       "0  tv future hand viewer home theatre system plas...  \n",
       "1  worldcom bos left book former worldcom bos ber...  \n",
       "2  tiger wary farrell gamble leicester rushed bid...  \n",
       "3  yeading newcastle fa cup premiership newcastle...  \n",
       "4  ocean twelve raid box office ocean twelve crim...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatize tokenized text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "dat['text_lemmatized'] = dat['text_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "# Detokenize the data\n",
    "dat['text_lemmatized'] = dat['text_lemmatized'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))\n",
    "\n",
    "dat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Bag-of-Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 451)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(max_df=0.15, min_df=0.05, max_features=3000, ngram_range=(1,2))\n",
    "text_counts = count_vect.fit_transform(dat['text_lemmatized'].tolist())\n",
    "text_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = text_counts.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count_vect.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest frequency words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('film', 1113),\n",
       " ('music', 835),\n",
       " ('labour', 796),\n",
       " ('party', 778),\n",
       " ('sale', 734)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The highest frequency words:')\n",
    "words_freq[0:5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest frequency words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('take', 126),\n",
       " ('tough', 125),\n",
       " ('decided', 125),\n",
       " ('remains', 124),\n",
       " ('13', 121)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The lowest frequency words:') \n",
    "words_freq[-5:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names=np.array(count_vect.get_feature_names())\n",
    "len(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 451 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9    ...  441  442  443  \\\n",
       "0       0    0    0    0    0    0    0    0    0    0  ...    0    0    1   \n",
       "1       0    0    0    0    0    0    0    1    0    1  ...    0    0    0   \n",
       "2       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3       0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4       0    0    0    0    0    0    0    0    1    0  ...    0    0    0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2220    0    1    0    0    0    0    1    0    0    0  ...    0    0    0   \n",
       "2221    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2222    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2223    0    0    0    0    0    0    0    0    0    0  ...    1    0    0   \n",
       "2224    0    0    1    0    0    1    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "      444  445  446  447  448  449  450  \n",
       "0       0    0    0    0    0    0    0  \n",
       "1       0    0    0    0    0    0    0  \n",
       "2       0    0    0    0    0    0    0  \n",
       "3       0    2    0    0    0    0    0  \n",
       "4       0    2    0    0    0    0    1  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "2220    0    0    0    0    0    0    0  \n",
       "2221    0    0    0    0    0    0    0  \n",
       "2222    0    0    1    0    0    0    0  \n",
       "2223    0    0    0    0    0    1    0  \n",
       "2224    0    0    0    1    0    0    0  \n",
       "\n",
       "[2225 rows x 451 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(text_counts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(use_idf=True, smooth_idf=True)\n",
    "text_tfidf = tfidf_transformer.fit_transform(text_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.63278772, 3.56046706, 3.37042345, 3.9630294 , 3.62655717,\n",
       "       3.97176308, 3.8176124 , 3.24837602, 3.65171573, 3.76631911,\n",
       "       2.95856855, 3.02778892, 3.81012173, 3.52617798, 3.87167962,\n",
       "       3.6453665 , 3.16278409, 3.73814823, 2.89682054, 3.73814823,\n",
       "       3.84042708, 3.67752361, 3.15888545, 3.95437134, 3.92883804,\n",
       "       3.89577718, 3.40964417, 3.6906817 , 3.28301151, 3.6080951 ,\n",
       "       3.51500468, 3.46093746, 3.85593127, 3.13201243, 3.36562728,\n",
       "       3.84042708, 3.98057371, 3.86377445, 3.73814823, 3.86377445,\n",
       "       3.9457876 , 3.6080951 , 3.54890623, 3.54317556, 3.54890623,\n",
       "       3.90394049, 3.68408101, 3.67752361, 3.75213447, 3.2786159 ,\n",
       "       3.89577718, 3.80268675, 3.22732261, 3.58996772, 3.92883804,\n",
       "       3.5094645 , 3.87964779, 3.89577718, 3.93727691, 3.83276421,\n",
       "       3.84814913, 3.92046979, 3.92883804, 3.1472799 , 2.99093383,\n",
       "       3.95437134, 3.86377445, 2.99093383, 3.69732624, 3.4556881 ,\n",
       "       3.13961703, 3.43496197, 3.98057371, 3.75213447, 3.92883804,\n",
       "       3.53747754, 3.5318118 , 3.76631911, 3.60201606, 3.23990139,\n",
       "       3.98057371, 3.04500105, 3.49847538, 3.93727691, 3.54890623,\n",
       "       3.69732624, 3.89577718, 3.71752895, 3.12071287, 3.84814913,\n",
       "       3.57216309, 3.53747754, 3.6080951 , 3.59597374, 3.5094645 ,\n",
       "       3.62655717, 3.66453641, 3.97176308, 3.68408101, 3.52057573,\n",
       "       3.55466994, 3.63278772, 3.52617798, 3.4196945 , 3.38495155,\n",
       "       3.04153484, 3.87964779, 3.9457876 , 3.46621452, 3.92883804,\n",
       "       3.69732624, 3.33268312, 3.28742653, 3.93727691, 3.28301151,\n",
       "       3.68408101, 3.84814913, 3.34198552, 3.11697455, 3.3373235 ,\n",
       "       3.9630294 , 3.03120773, 3.41465671, 3.84042708, 3.81012173,\n",
       "       3.02098618, 3.65171573, 3.97176308, 3.84042708, 3.6453665 ,\n",
       "       3.61421133, 2.92106415, 3.57216309, 3.86377445, 2.97786175,\n",
       "       3.9457876 , 3.17457204, 3.1358075 , 3.4556881 , 3.39969383,\n",
       "       3.6203652 , 3.68408101, 3.28301151, 3.20263   , 3.86377445,\n",
       "       3.97176308, 3.18250859, 3.05547235, 3.50395484, 3.91217099,\n",
       "       3.13201243, 3.91217099, 3.98057371, 3.6906817 , 3.73814823,\n",
       "       3.58996772, 3.63278772, 3.7451169 , 3.50395484, 3.6080951 ,\n",
       "       3.98057371, 3.90394049, 3.08756067, 3.07317193, 3.26122416,\n",
       "       3.49847538, 3.9630294 , 3.24412973, 3.97176308, 3.81012173,\n",
       "       3.92046979, 3.59597374, 3.51500468, 3.61421133, 3.22732261,\n",
       "       3.9630294 , 3.48221486, 3.67752361, 3.97176308, 3.6203652 ,\n",
       "       3.80268675, 3.07674975, 3.49302577, 3.6080951 , 3.65171573,\n",
       "       3.86377445, 3.9457876 , 3.8176124 , 3.26988222, 3.39969383,\n",
       "       3.63905733, 3.3561034 , 3.49847538, 3.98057371, 3.32806418,\n",
       "       2.90282656, 3.87167962, 3.49847538, 3.16669799, 3.80268675,\n",
       "       3.54317556, 3.76631911, 3.50395484, 3.65171573, 3.36562728,\n",
       "       3.90394049, 3.37524274, 3.85593127, 3.1358075 , 3.5318118 ,\n",
       "       3.67752361, 3.6906817 , 2.96816862, 3.60201606, 3.75920164,\n",
       "       3.8176124 , 3.80268675, 2.97786175, 3.24412973, 3.26122416,\n",
       "       3.37524274, 3.34198552, 3.23569086, 3.69732624, 3.80268675,\n",
       "       3.84814913, 3.38008536, 3.37042345, 3.18650061, 3.7734876 ,\n",
       "       3.66453641, 3.40964417, 3.6080951 , 3.23990139, 3.85593127,\n",
       "       3.92883804, 3.03808061, 3.46621452, 3.73814823, 3.360854  ,\n",
       "       3.72435491, 3.63278772, 3.04500105, 3.00085127, 3.25264042,\n",
       "       3.19050864, 3.11325015, 3.6906817 , 3.51500468, 3.26122416,\n",
       "       3.79530665, 3.57806282, 3.52617798, 3.38984154, 3.9457876 ,\n",
       "       3.38495155, 3.87167962, 3.03808061, 3.85593127, 3.38008536,\n",
       "       3.78798061, 3.49302577, 3.84814913, 3.28742653, 3.58399755,\n",
       "       3.87167962, 3.8176124 , 3.52617798, 3.08394403, 2.89682054,\n",
       "       3.69732624, 3.70401523, 3.23990139, 3.52057573, 3.30979883,\n",
       "       3.16278409, 3.44527134, 3.58399755, 3.92883804, 3.6080951 ,\n",
       "       3.82515961, 3.24412973, 3.23149798, 3.38984154, 3.60201606,\n",
       "       3.58996772, 3.85593127, 3.89577718, 3.14344112, 3.20670332,\n",
       "       3.88767997, 3.48760571, 3.86377445, 3.40964417, 3.8176124 ,\n",
       "       3.51500468, 3.71752895, 3.9457876 , 3.32806418, 3.35137526,\n",
       "       3.50395484, 3.66453641, 3.75213447, 3.65810552, 3.84042708,\n",
       "       3.39475555, 3.36562728, 3.9457876 , 3.78070785, 3.62655717,\n",
       "       3.25264042, 3.56046706, 3.57216309, 3.91217099, 3.59597374,\n",
       "       3.85593127, 3.97176308, 3.24837602, 3.8176124 , 3.87964779,\n",
       "       3.91217099, 2.90282656, 3.68408101, 3.75213447, 3.41465671,\n",
       "       3.9630294 , 3.51500468, 3.5094645 , 3.93727691, 3.32346647,\n",
       "       3.87167962, 3.76631911, 3.1985732 , 3.57806282, 2.97462025,\n",
       "       3.89577718, 3.67752361, 3.78798061, 3.9630294 , 2.92413635,\n",
       "       3.75213447, 3.46621452, 3.08034042, 3.30979883, 3.80268675,\n",
       "       3.30979883, 3.89577718, 3.73814823, 3.98057371, 3.3373235 ,\n",
       "       3.84042708, 3.53747754, 3.13201243, 3.66453641, 2.92413635,\n",
       "       3.67100893, 3.15500195, 3.83276421, 3.83276421, 3.54317556,\n",
       "       3.46621452, 3.4556881 , 3.08034042, 3.82515961, 3.93727691,\n",
       "       3.75920164, 3.52057573, 3.84042708, 3.15113347, 3.97176308,\n",
       "       3.93727691, 3.79530665, 3.87167962, 3.6453665 , 3.98057371,\n",
       "       3.95437134, 2.93340999, 3.79530665, 3.69732624, 3.93727691,\n",
       "       3.61421133, 3.87167962, 3.7451169 , 3.9630294 , 3.10215947,\n",
       "       3.67100893, 3.72435491, 3.86377445, 3.78798061, 3.13961703,\n",
       "       3.00751796, 3.84042708, 3.41465671, 3.84042708, 3.85593127,\n",
       "       3.87964779, 3.76631911, 3.32806418, 3.52617798, 3.85593127,\n",
       "       3.38008536, 3.3188898 , 3.15888545, 3.9457876 , 3.15500195,\n",
       "       3.49847538, 3.72435491, 3.71074926, 3.18650061, 3.21902381,\n",
       "       3.97176308, 3.43496197, 3.75920164, 3.76631911, 2.96175834,\n",
       "       3.71074926, 3.49847538, 2.99422873, 3.48221486, 3.66453641,\n",
       "       3.5094645 , 3.63278772, 3.98057371, 3.59597374, 3.91217099,\n",
       "       3.48221486, 3.59597374, 3.45046616, 3.38008536, 3.67100893,\n",
       "       3.60201606, 3.4556881 , 3.27423953, 3.80268675, 3.360854  ,\n",
       "       3.73814823, 3.71752895, 3.61421133, 3.79530665, 3.90394049,\n",
       "       3.78070785, 3.75213447, 3.50395484, 3.24837602, 3.50395484,\n",
       "       3.97176308, 3.7734876 , 3.58996772, 3.68408101, 3.98057371,\n",
       "       3.86377445])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(451,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['particularly' 'reason' 'tony' 'themselves' 'similar' 'tony blair' 'take'\n",
      " 'reach' 'bbc news' 'despite' 'remain' 'simply' 'brought' 'due' 'told bbc'\n",
      " 'potential' 'thursday' '30' 'instead' 'according']\n",
      "Features with highest tfidf: \n",
      "['book' 'goal' 'charles' 'moment' 'net' 'report' 'election' 'project' 'pc'\n",
      " 'radio' 'right' 'child' 'woman' 'card' 'mobile' '50' 'music' 'award'\n",
      " 'bank' 'film']\n"
     ]
    }
   ],
   "source": [
    "# find maximum value for each of the features over dataset:\n",
    "max_value = text_tfidf.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[:20]]))\n",
    "\n",
    "print(\"Features with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.144441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086711</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.169576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 451 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1         2    3    4         5         6         7    \\\n",
       "0     0.0  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "1     0.0  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.124578   \n",
       "2     0.0  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "3     0.0  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "4     0.0  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "...   ...       ...       ...  ...  ...       ...       ...       ...   \n",
       "2220  0.0  0.062881  0.000000  0.0  0.0  0.000000  0.067423  0.000000   \n",
       "2221  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "2222  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "2223  0.0  0.000000  0.000000  0.0  0.0  0.000000  0.000000  0.000000   \n",
       "2224  0.0  0.000000  0.143901  0.0  0.0  0.169576  0.000000  0.000000   \n",
       "\n",
       "           8         9    ...       441  442       443  444       445  \\\n",
       "0     0.000000  0.000000  ...  0.000000  0.0  0.049065  0.0  0.000000   \n",
       "1     0.000000  0.144441  ...  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "2     0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "3     0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.250514   \n",
       "4     0.158266  0.000000  ...  0.000000  0.0  0.000000  0.0  0.344275   \n",
       "...        ...       ...  ...       ...  ...       ...  ...       ...   \n",
       "2220  0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "2221  0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "2222  0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "2223  0.000000  0.000000  ...  0.081735  0.0  0.000000  0.0  0.000000   \n",
       "2224  0.000000  0.000000  ...  0.000000  0.0  0.000000  0.0  0.000000   \n",
       "\n",
       "           446       447  448       449       450  \n",
       "0     0.000000  0.000000  0.0  0.000000  0.000000  \n",
       "1     0.000000  0.000000  0.0  0.000000  0.000000  \n",
       "2     0.000000  0.000000  0.0  0.000000  0.000000  \n",
       "3     0.000000  0.000000  0.0  0.000000  0.000000  \n",
       "4     0.000000  0.000000  0.0  0.000000  0.167457  \n",
       "...        ...       ...  ...       ...       ...  \n",
       "2220  0.000000  0.000000  0.0  0.000000  0.000000  \n",
       "2221  0.000000  0.000000  0.0  0.000000  0.000000  \n",
       "2222  0.153692  0.000000  0.0  0.000000  0.000000  \n",
       "2223  0.000000  0.000000  0.0  0.086711  0.000000  \n",
       "2224  0.000000  0.153275  0.0  0.000000  0.000000  \n",
       "\n",
       "[2225 rows x 451 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(text_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract keywords using TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of top keywords to extract\n",
    "topn = 7\n",
    "results =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, doc in dat['text_lemmatized'].items():\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=text_tfidf[idx]\n",
    "    \n",
    "    temp = pd.DataFrame(zip(tf_idf_vector.tocoo().col, tf_idf_vector.tocoo().data),columns=['feature_number','tf_idf'])\n",
    "    temp.sort_values('tf_idf', ascending = False, inplace = True)\n",
    "    \n",
    "    #use only topn items from vector\n",
    "     \n",
    "    topn_items = temp[:topn]\n",
    "\n",
    "    tf_idf = []\n",
    "    word = []\n",
    "\n",
    "    for index, row in topn_items.iterrows():\n",
    "        #print(int(row['feature_number']))\n",
    "        fname = feature_names[int(row['feature_number'])]\n",
    "        word.append(fname)\n",
    "        tf_idf.append(round(row['tf_idf'], 3))\n",
    "\n",
    "    result = dict(zip(word, tf_idf))\n",
    "    \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_nostop</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>keywords + tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, in, the, hands, of, viewers, with...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "      <td>{'tv': 0.715, 'network': 0.271, 'technology': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>[worldcom, boss, left, books, former, worldcom...</td>\n",
       "      <td>worldcom bos left book former worldcom bos ber...</td>\n",
       "      <td>{'bos': 0.443, 'book': 0.302, 'admitted': 0.30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>[tigers, wary, of, farrell, gamble, leicester,...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, rus...</td>\n",
       "      <td>tiger wary farrell gamble leicester rushed bid...</td>\n",
       "      <td>{'union': 0.474, 'involved': 0.329, 'club': 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, in, fa, cup, premie...</td>\n",
       "      <td>[yeading, newcastle, fa, cup, premiership, new...</td>\n",
       "      <td>yeading newcastle fa cup premiership newcastle...</td>\n",
       "      <td>{'cup': 0.466, 'united': 0.454, 'meet': 0.358,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, s, twelve, raids, box, office, ocean, ...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "      <td>{'office': 0.42, 'robert': 0.345, 'weekend': 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [tv, future, in, the, hands, of, viewers, with...   \n",
       "1  [worldcom, boss, left, books, alone, former, w...   \n",
       "2  [tigers, wary, of, farrell, gamble, leicester,...   \n",
       "3  [yeading, face, newcastle, in, fa, cup, premie...   \n",
       "4  [ocean, s, twelve, raids, box, office, ocean, ...   \n",
       "\n",
       "                                         text_nostop  \\\n",
       "0  [tv, future, hands, viewers, home, theatre, sy...   \n",
       "1  [worldcom, boss, left, books, former, worldcom...   \n",
       "2  [tigers, wary, farrell, gamble, leicester, rus...   \n",
       "3  [yeading, newcastle, fa, cup, premiership, new...   \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  tv future hand viewer home theatre system plas...   \n",
       "1  worldcom bos left book former worldcom bos ber...   \n",
       "2  tiger wary farrell gamble leicester rushed bid...   \n",
       "3  yeading newcastle fa cup premiership newcastle...   \n",
       "4  ocean twelve raid box office ocean twelve crim...   \n",
       "\n",
       "                                    keywords + tfidf  \n",
       "0  {'tv': 0.715, 'network': 0.271, 'technology': ...  \n",
       "1  {'bos': 0.443, 'book': 0.302, 'admitted': 0.30...  \n",
       "2  {'union': 0.474, 'involved': 0.329, 'club': 0....  \n",
       "3  {'cup': 0.466, 'united': 0.454, 'meet': 0.358,...  \n",
       "4  {'office': 0.42, 'robert': 0.345, 'weekend': 0...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add new columns\n",
    "\n",
    "dat['keywords + tfidf'] = results\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Association Rules Mining on keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-024f90cac9da>:9: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  full_list=pd.Series([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>Generate the Frequent Itemsets using Apriori Algorithm and set Support threshold as 0.01\n",
      "    support  itemsets\n",
      "0  0.076404    (film)\n",
      "1  0.048539  (labour)\n",
      "2  0.046742   (music)\n",
      "3  0.046742   (award)\n",
      "4  0.046292    (sale)\n",
      "\n",
      ">>> Generate association rules using Confidence threshold\n",
      "       antecedents consequents  antecedent support  consequent support  \\\n",
      "38  (actor, award)      (film)            0.016180            0.076404   \n",
      "27         (phone)    (mobile)            0.034607            0.036854   \n",
      "3          (actor)      (film)            0.035506            0.076404   \n",
      "26        (mobile)     (phone)            0.036854            0.034607   \n",
      "36   (film, award)     (actor)            0.022921            0.035506   \n",
      "\n",
      "     support  confidence       lift  leverage  conviction  \n",
      "38  0.012584    0.777778  10.179739  0.011348    4.156180  \n",
      "27  0.023820    0.688312  18.676750  0.022545    3.090094  \n",
      "3   0.024270    0.683544   8.946389  0.021557    2.918562  \n",
      "26  0.023820    0.646341  18.676750  0.022545    2.729733  \n",
      "36  0.012584    0.549020  15.462894  0.011770    2.138661  \n",
      "\n",
      ">>> Generate association rules using Lift threshold\n",
      "     antecedents consequents  antecedent support  consequent support  \\\n",
      "0       (mobile)     (phone)            0.036854            0.034607   \n",
      "1        (phone)    (mobile)            0.034607            0.036854   \n",
      "2       (growth)   (economy)            0.030112            0.032360   \n",
      "3      (economy)    (growth)            0.032360            0.030112   \n",
      "8  (film, award)     (actor)            0.022921            0.035506   \n",
      "\n",
      "    support  confidence       lift  leverage  conviction  \n",
      "0  0.023820    0.646341  18.676750  0.022545    2.729733  \n",
      "1  0.023820    0.688312  18.676750  0.022545    3.090094  \n",
      "2  0.015281    0.507463  15.682007  0.014306    1.964603  \n",
      "3  0.015281    0.472222  15.682007  0.014306    1.837682  \n",
      "8  0.012584    0.549020  15.462894  0.011770    2.138661  \n",
      "\n",
      ">>> Generate association rules using both Lift and Confidence threshold\n",
      "       antecedents consequents  antecedent support  consequent support  \\\n",
      "38  (actor, award)      (film)            0.016180            0.076404   \n",
      "27         (phone)    (mobile)            0.034607            0.036854   \n",
      "26        (mobile)     (phone)            0.036854            0.034607   \n",
      "36   (film, award)     (actor)            0.022921            0.035506   \n",
      "\n",
      "     support  confidence       lift  leverage  conviction  \n",
      "38  0.012584    0.777778  10.179739  0.011348    4.156180  \n",
      "27  0.023820    0.688312  18.676750  0.022545    3.090094  \n",
      "26  0.023820    0.646341  18.676750  0.022545    2.729733  \n",
      "36  0.012584    0.549020  15.462894  0.011770    2.138661  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-024f90cac9da>:41: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  print(rules_c[ (rules_c['lift'] >= 10) &\n"
     ]
    }
   ],
   "source": [
    "dat['keywords'] = dat['keywords + tfidf'].apply(lambda x: list(x.keys()))\n",
    "\n",
    "with open(\"ARMKeywords.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(dat['keywords'])\n",
    "\n",
    "ARMdata = pd.read_csv('ARMKeywords.csv', header = None)\n",
    "\n",
    "full_list=pd.Series([])\n",
    "for col in ARMdata:\n",
    "    full_list = full_list.append(ARMdata[col].dropna())\n",
    "\n",
    "y = full_list.value_counts().head(50).to_frame()\n",
    "\n",
    "trans = []\n",
    "for i in range(0, 2225):\n",
    "    trans.append([str(ARMdata.values[i,j]) for j in range(0, 7)])\n",
    "\n",
    "# conveting it into an numpy array\n",
    "trans = np.array(trans)\n",
    "\n",
    "# transform into one-hot encoded NumPy boolean array\n",
    "te = TransactionEncoder()\n",
    "data_encoded = te.fit_transform(trans)\n",
    "data_encoded = pd.DataFrame(data_encoded, columns = te.columns_)\n",
    "\n",
    "data_encoded = data_encoded.loc[:, y.index]\n",
    "\n",
    "# Generate the Frequent Itemsets using Apriori Algorithm and set Support threshold as 0.01\n",
    "frequent_itemsets=apriori(data_encoded, min_support = 0.01, use_colnames = True)\n",
    "print(\"\\n>>>Generate the Frequent Itemsets using Apriori Algorithm and set Support threshold as 0.01\")\n",
    "print(frequent_itemsets.head())\n",
    "rules_c = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "rules_c.sort_values('confidence', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Confidence threshold\")\n",
    "print(rules_c.head(5))\n",
    "rules_l = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=11)\n",
    "rules_l.sort_values('lift', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Lift threshold\")\n",
    "print(rules_l.head(5))\n",
    "print(\"\\n>>> Generate association rules using both Lift and Confidence threshold\")\n",
    "print(rules_c[ (rules_c['lift'] >= 10) &\n",
    "       (rules_c['confidence'] >= 0.4).head(5) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-64b5fca35b7b>:9: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tech_list=pd.Series([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Generate association rules using Confidence threshold\n",
      "                                 antecedents consequents  antecedent support  \\\n",
      "120                          (mobile, using)     (phone)            0.012469   \n",
      "153                          (user, website)      (site)            0.014963   \n",
      "118                        (mobile, message)     (phone)            0.014963   \n",
      "277  (technology, consumer, device, digital)     (video)            0.014963   \n",
      "274   (technology, video, consumer, digital)    (device)            0.014963   \n",
      "\n",
      "     consequent support   support  confidence       lift  leverage  conviction  \n",
      "120            0.157107  0.012469         1.0   6.365079  0.010510         inf  \n",
      "153            0.129676  0.014963         1.0   7.711538  0.013022         inf  \n",
      "118            0.157107  0.014963         1.0   6.365079  0.012612         inf  \n",
      "277            0.092269  0.014963         1.0  10.837838  0.013582         inf  \n",
      "274            0.124688  0.014963         1.0   8.020000  0.013097         inf  \n",
      "\n",
      ">>> Generate association rules using Lift threshold\n",
      "                      antecedents                     consequents  \\\n",
      "48  (technology, device, digital)               (video, consumer)   \n",
      "49              (video, consumer)   (technology, device, digital)   \n",
      "55            (consumer, digital)     (technology, video, device)   \n",
      "42    (technology, video, device)             (consumer, digital)   \n",
      "52               (video, digital)  (technology, consumer, device)   \n",
      "\n",
      "    antecedent support  consequent support   support  confidence       lift  \\\n",
      "48            0.017456            0.024938  0.014963    0.857143  34.371429   \n",
      "49            0.024938            0.017456  0.014963    0.600000  34.371429   \n",
      "55            0.029925            0.014963  0.014963    0.500000  33.416667   \n",
      "42            0.014963            0.029925  0.014963    1.000000  33.416667   \n",
      "52            0.027431            0.017456  0.014963    0.545455  31.246753   \n",
      "\n",
      "    leverage  conviction  \n",
      "48  0.014527    6.825436  \n",
      "49  0.014527    2.456359  \n",
      "55  0.014515    1.970075  \n",
      "42  0.014515         inf  \n",
      "52  0.014484    2.161596  \n",
      "\n",
      ">>> Generate association rules using both Lift and Confidence threshold\n",
      "                       antecedents          consequents  antecedent support  \\\n",
      "281    (technology, video, device)  (consumer, digital)            0.014963   \n",
      "206                (release, sold)             (launch)            0.012469   \n",
      "207                 (launch, sold)            (release)            0.012469   \n",
      "268                (video, device)  (consumer, digital)            0.019950   \n",
      "287  (technology, device, digital)    (video, consumer)            0.017456   \n",
      "\n",
      "     consequent support   support  confidence       lift  leverage  conviction  \n",
      "281            0.029925  0.014963    1.000000  33.416667  0.014515         inf  \n",
      "206            0.037406  0.012469    1.000000  26.733333  0.012002         inf  \n",
      "207            0.047382  0.012469    1.000000  21.105263  0.011878         inf  \n",
      "268            0.029925  0.017456    0.875000  29.239583  0.016859    7.760599  \n",
      "287            0.024938  0.014963    0.857143  34.371429  0.014527    6.825436  \n"
     ]
    }
   ],
   "source": [
    "techdf = pd.DataFrame(dat.loc[dat['category'] == \"tech\"])\n",
    "\n",
    "with open(\"ARMTechKeywords.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(techdf['keywords'])\n",
    "\n",
    "ARMTechdata = pd.read_csv('ARMTechKeywords.csv', header = None)\n",
    "\n",
    "tech_list=pd.Series([])\n",
    "for col in ARMTechdata:\n",
    "    tech_list = tech_list.append(ARMTechdata[col].dropna())\n",
    "    \n",
    "y = tech_list.value_counts().head(50).to_frame()\n",
    "\n",
    "trans = []\n",
    "for i in range(0, 401):\n",
    "    trans.append([str(ARMTechdata.values[i,j]) for j in range(0, 7)])\n",
    "\n",
    "# conveting it into an numpy array\n",
    "trans = np.array(trans)\n",
    "\n",
    "te = TransactionEncoder()\n",
    "data_encoded = te.fit_transform(trans)\n",
    "data_encoded = pd.DataFrame(data_encoded, columns = te.columns_)\n",
    "data_encoded = data_encoded.loc[:, y.index]\n",
    "\n",
    "frequent_itemsets=apriori(data_encoded, min_support = 0.01, use_colnames = True)\n",
    "rules_c = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "rules_c.sort_values('confidence', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Confidence threshold\")\n",
    "print(rules_c.head(5))\n",
    "rules_l = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=11)\n",
    "rules_l.sort_values('lift', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Lift threshold\")\n",
    "print(rules_l.head(5))\n",
    "print(\"\\n>>> Generate association rules using both Lift and Confidence threshold\")\n",
    "print(rules_c[ (rules_c['lift'] >= 20) &\n",
    "       (rules_c['confidence'] >= 0.5) ].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-5eb1acbccb60>:9: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  busi_list=pd.Series([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Generate association rules using Confidence threshold\n",
      "                    antecedents consequents  antecedent support  \\\n",
      "0              (share, quarter)    (profit)            0.005882   \n",
      "44  (european, growth, economy)  (economic)            0.005882   \n",
      "23                  (job, rate)    (growth)            0.009804   \n",
      "47   (analyst, growth, economy)      (rate)            0.005882   \n",
      "25          (spending, quarter)    (growth)            0.007843   \n",
      "\n",
      "    consequent support   support  confidence       lift  leverage  conviction  \n",
      "0             0.129412  0.005882         1.0   7.727273  0.005121         inf  \n",
      "44            0.105882  0.005882         1.0   9.444444  0.005260         inf  \n",
      "23            0.125490  0.009804         1.0   7.968750  0.008574         inf  \n",
      "47            0.098039  0.005882         1.0  10.200000  0.005306         inf  \n",
      "25            0.125490  0.007843         1.0   7.968750  0.006859         inf  \n",
      "\n",
      ">>> Generate association rules using Lift threshold\n",
      "            antecedents          consequents  antecedent support  \\\n",
      "38      (rate, quarter)       (growth, rise)            0.017647   \n",
      "35       (growth, rise)      (rate, quarter)            0.019608   \n",
      "23   (european, growth)  (economic, economy)            0.005882   \n",
      "24  (economic, economy)   (european, growth)            0.045098   \n",
      "28   (analyst, economy)       (growth, rate)            0.005882   \n",
      "\n",
      "    consequent support   support  confidence       lift  leverage  conviction  \n",
      "38            0.019608  0.009804    0.555556  28.333333  0.009458    2.205882  \n",
      "35            0.017647  0.009804    0.500000  28.333333  0.009458    1.964706  \n",
      "23            0.045098  0.005882    1.000000  22.173913  0.005617         inf  \n",
      "24            0.005882  0.005882    0.130435  22.173913  0.005617    1.143235  \n",
      "28            0.047059  0.005882    1.000000  21.250000  0.005606         inf  \n",
      "\n",
      ">>> Generate association rules using both Lift and Confidence threshold\n",
      "           antecedents          consequents  antecedent support  \\\n",
      "45  (european, growth)  (economic, economy)            0.005882   \n",
      "37          (cut, tax)           (spending)            0.005882   \n",
      "38  (spending, figure)           (consumer)            0.005882   \n",
      "39  (consumer, figure)           (spending)            0.005882   \n",
      "50  (analyst, economy)       (growth, rate)            0.005882   \n",
      "\n",
      "    consequent support   support  confidence       lift  leverage  conviction  \n",
      "45            0.045098  0.005882         1.0  22.173913  0.005617         inf  \n",
      "37            0.047059  0.005882         1.0  21.250000  0.005606         inf  \n",
      "38            0.047059  0.005882         1.0  21.250000  0.005606         inf  \n",
      "39            0.047059  0.005882         1.0  21.250000  0.005606         inf  \n",
      "50            0.047059  0.005882         1.0  21.250000  0.005606         inf  \n"
     ]
    }
   ],
   "source": [
    "busidf = pd.DataFrame(dat.loc[dat['category'] == \"business\"])\n",
    "\n",
    "with open(\"ARMBusiKeywords.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(busidf['keywords'])\n",
    "\n",
    "ARMBusidata = pd.read_csv('ARMBusiKeywords.csv', header = None)\n",
    "\n",
    "busi_list=pd.Series([])\n",
    "for col in ARMBusidata:\n",
    "    busi_list = busi_list.append(ARMBusidata[col].dropna())\n",
    "\n",
    "y = busi_list.value_counts().head(50).to_frame()\n",
    "\n",
    "trans = []\n",
    "for i in range(0, 510):\n",
    "    trans.append([str(ARMBusidata.values[i,j]) for j in range(0, 7)])\n",
    "\n",
    "# conveting it into an numpy array\n",
    "trans = np.array(trans)\n",
    "te = TransactionEncoder()\n",
    "data_encoded = te.fit_transform(trans)\n",
    "data_encoded = pd.DataFrame(data_encoded, columns = te.columns_)\n",
    "\n",
    "data_encoded = data_encoded.loc[:, y.index]\n",
    "\n",
    "frequent_itemsets=apriori(data_encoded, min_support = 0.005, use_colnames = True)\n",
    "rules_c = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.8)\n",
    "rules_c.sort_values('confidence', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Confidence threshold\")\n",
    "print(rules_c.head(5))\n",
    "rules_l = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=15)\n",
    "rules_l.sort_values('lift', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Lift threshold\")\n",
    "print(rules_l.head(5))\n",
    "\n",
    "print(\"\\n>>> Generate association rules using both Lift and Confidence threshold\")\n",
    "print(rules_c[ (rules_c['lift'] >= 20) &\n",
    "       (rules_c['confidence'] >= 0.4) ].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-ef73affcd117>:9: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  sport_list=pd.Series([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Generate association rules using Confidence threshold\n",
      "               antecedents consequents  antecedent support  \\\n",
      "4              (six, wale)    (nation)            0.013699   \n",
      "2           (six, ireland)    (nation)            0.017613   \n",
      "5  (wale, france, ireland)   (england)            0.015656   \n",
      "1        (nation, england)       (six)            0.023483   \n",
      "0        (france, ireland)   (england)            0.029354   \n",
      "\n",
      "   consequent support   support  confidence       lift  leverage  conviction  \n",
      "4            0.052838  0.013699    1.000000  18.925926  0.012975         inf  \n",
      "2            0.052838  0.015656    0.888889  16.823045  0.014725    8.524462  \n",
      "5            0.152642  0.013699    0.875000   5.732372  0.011309    6.778865  \n",
      "1            0.060665  0.019569    0.833333  13.736559  0.018145    5.636008  \n",
      "0            0.152642  0.023483    0.800000   5.241026  0.019003    4.236791  \n",
      "\n",
      ">>> Generate association rules using Lift threshold\n",
      "          antecedents     consequents  antecedent support  consequent support  \\\n",
      "14        (six, wale)        (nation)            0.013699            0.052838   \n",
      "17           (nation)     (six, wale)            0.052838            0.013699   \n",
      "6      (six, ireland)        (nation)            0.017613            0.052838   \n",
      "9            (nation)  (six, ireland)            0.052838            0.017613   \n",
      "3   (nation, england)           (six)            0.023483            0.060665   \n",
      "\n",
      "     support  confidence       lift  leverage  conviction  \n",
      "14  0.013699    1.000000  18.925926  0.012975         inf  \n",
      "17  0.013699    0.259259  18.925926  0.012975    1.331507  \n",
      "6   0.015656    0.888889  16.823045  0.014725    8.524462  \n",
      "9   0.015656    0.296296  16.823045  0.014725    1.396024  \n",
      "3   0.019569    0.833333  13.736559  0.018145    5.636008  \n",
      "\n",
      ">>> Generate association rules using both Lift and Confidence threshold\n",
      "         antecedents consequents  antecedent support  consequent support  \\\n",
      "4        (six, wale)    (nation)            0.013699            0.052838   \n",
      "2     (six, ireland)    (nation)            0.017613            0.052838   \n",
      "1  (nation, england)       (six)            0.023483            0.060665   \n",
      "3  (nation, ireland)       (six)            0.019569            0.060665   \n",
      "\n",
      "    support  confidence       lift  leverage  conviction  \n",
      "4  0.013699    1.000000  18.925926  0.012975         inf  \n",
      "2  0.015656    0.888889  16.823045  0.014725    8.524462  \n",
      "1  0.019569    0.833333  13.736559  0.018145    5.636008  \n",
      "3  0.015656    0.800000  13.187097  0.014468    4.696673  \n"
     ]
    }
   ],
   "source": [
    "sportdf = pd.DataFrame(dat.loc[dat['category'] == \"sport\"])\n",
    "\n",
    "with open(\"ARMSportKeywords.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sportdf['keywords'])\n",
    "\n",
    "ARMSportdata = pd.read_csv('ARMSportKeywords.csv', header = None)\n",
    "\n",
    "sport_list=pd.Series([])\n",
    "for col in ARMSportdata:\n",
    "    sport_list = sport_list.append(ARMSportdata[col].dropna())\n",
    "\n",
    "y = sport_list.value_counts().head(50).to_frame()\n",
    "\n",
    "trans = []\n",
    "for i in range(0, 511):\n",
    "    trans.append([str(ARMSportdata.values[i,j]) for j in range(0, 7)])\n",
    "\n",
    "# conveting it into an numpy array\n",
    "trans = np.array(trans)\n",
    "te = TransactionEncoder()\n",
    "data_encoded = te.fit_transform(trans)\n",
    "data_encoded = pd.DataFrame(data_encoded, columns = te.columns_)\n",
    "\n",
    "data_encoded = data_encoded.loc[:, y.index]\n",
    "\n",
    "frequent_itemsets=apriori(data_encoded, min_support = 0.01, use_colnames = True)\n",
    "rules_c = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.8)\n",
    "rules_c.sort_values('confidence', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Confidence threshold\")\n",
    "print(rules_c.head(5))\n",
    "rules_l = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=10)\n",
    "rules_l.sort_values('lift', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Lift threshold\")\n",
    "print(rules_l.head(5))\n",
    "\n",
    "print(\"\\n>>> Generate association rules using both Lift and Confidence threshold\")\n",
    "print(rules_c[ (rules_c['lift'] >= 11) &\n",
    "       (rules_c['confidence'] >= 0.5) ].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-07d56355b070>:9: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  entertainment_list=pd.Series([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Generate association rules using Confidence threshold\n",
      "                  antecedents consequents  antecedent support  \\\n",
      "139    (role, british, actor)      (film)            0.010363   \n",
      "116  (film, included, winner)     (award)            0.012953   \n",
      "188  (actor, dollar, charles)  (director)            0.010363   \n",
      "83          (weekend, taking)    (office)            0.015544   \n",
      "163   (film, release, taking)    (office)            0.012953   \n",
      "\n",
      "     consequent support   support  confidence       lift  leverage  conviction  \n",
      "139            0.393782  0.010363         1.0   2.539474  0.006282         inf  \n",
      "116            0.238342  0.012953         1.0   4.195652  0.009866         inf  \n",
      "188            0.111399  0.010363         1.0   8.976744  0.009208         inf  \n",
      "83             0.062176  0.015544         1.0  16.083333  0.014578         inf  \n",
      "163            0.062176  0.012953         1.0  16.083333  0.012148         inf  \n",
      "\n",
      ">>> Generate association rules using Lift threshold\n",
      "                    antecedents                  consequents  \\\n",
      "49  (award, director, included)               (film, winner)   \n",
      "56               (film, winner)  (award, director, included)   \n",
      "51      (film, award, included)           (director, winner)   \n",
      "54           (director, winner)      (film, award, included)   \n",
      "53            (award, included)     (film, director, winner)   \n",
      "\n",
      "    antecedent support  consequent support   support  confidence       lift  \\\n",
      "49            0.012953            0.020725  0.010363    0.800000  38.600000   \n",
      "56            0.020725            0.012953  0.010363    0.500000  38.600000   \n",
      "51            0.015544            0.018135  0.010363    0.666667  36.761905   \n",
      "54            0.018135            0.015544  0.010363    0.571429  36.761905   \n",
      "53            0.023316            0.012953  0.010363    0.444444  34.311111   \n",
      "\n",
      "    leverage  conviction  \n",
      "49  0.010094    4.896373  \n",
      "56  0.010094    1.974093  \n",
      "51  0.010081    2.945596  \n",
      "54  0.010081    2.297064  \n",
      "53  0.010061    1.776684  \n",
      "\n",
      ">>> Generate association rules using both Lift and Confidence threshold\n",
      "                 antecedents consequents  antecedent support  \\\n",
      "83         (weekend, taking)    (office)            0.015544   \n",
      "163  (film, release, taking)    (office)            0.012953   \n",
      "81         (weekend, robert)    (office)            0.012953   \n",
      "80         (release, taking)    (office)            0.012953   \n",
      "78        (release, weekend)    (office)            0.012953   \n",
      "\n",
      "     consequent support   support  confidence       lift  leverage  conviction  \n",
      "83             0.062176  0.015544         1.0  16.083333  0.014578         inf  \n",
      "163            0.062176  0.012953         1.0  16.083333  0.012148         inf  \n",
      "81             0.062176  0.012953         1.0  16.083333  0.012148         inf  \n",
      "80             0.062176  0.012953         1.0  16.083333  0.012148         inf  \n",
      "78             0.062176  0.012953         1.0  16.083333  0.012148         inf  \n"
     ]
    }
   ],
   "source": [
    "entertainmentdf = pd.DataFrame(dat.loc[dat['category'] == \"entertainment\"])\n",
    "\n",
    "with open(\"ARMEntertainKeywords.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(entertainmentdf['keywords'])\n",
    "\n",
    "ARMEntertainmentdata = pd.read_csv('ARMEntertainKeywords.csv', header = None)\n",
    "\n",
    "entertainment_list=pd.Series([])\n",
    "for col in ARMEntertainmentdata:\n",
    "    entertainment_list = entertainment_list.append(ARMEntertainmentdata[col].dropna())\n",
    "\n",
    "y = entertainment_list.value_counts().head(50).to_frame()\n",
    "\n",
    "trans = []\n",
    "for i in range(0, 386):\n",
    "    trans.append([str(ARMEntertainmentdata.values[i,j]) for j in range(0, 7)])\n",
    "\n",
    "# conveting it into an numpy array\n",
    "trans = np.array(trans)\n",
    "te = TransactionEncoder()\n",
    "data_encoded = te.fit_transform(trans)\n",
    "data_encoded = pd.DataFrame(data_encoded, columns = te.columns_)\n",
    "\n",
    "data_encoded = data_encoded.loc[:, y.index]\n",
    "\n",
    "frequent_itemsets=apriori(data_encoded, min_support = 0.01, use_colnames = True)\n",
    "rules_c = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.8)\n",
    "rules_c.sort_values('confidence', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Confidence threshold\")\n",
    "print(rules_c.head(5))\n",
    "rules_l = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=19)\n",
    "rules_l.sort_values('lift', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Lift threshold\")\n",
    "print(rules_l.head(5))\n",
    "\n",
    "print(\"\\n>>> Generate association rules using both Lift and Confidence threshold\")\n",
    "print(rules_c[ (rules_c['lift'] >= 15) &\n",
    "       (rules_c['confidence'] >= 0.35) ].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-d698c7ed15a2>:9: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  politics_list=pd.Series([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Generate association rules using Confidence threshold\n",
      "                             antecedents consequents  antecedent support  \\\n",
      "107                (cut, spending, tory)       (tax)            0.014388   \n",
      "91     (prime minister, brown, election)     (prime)            0.011990   \n",
      "26                   (blair, chancellor)     (brown)            0.035971   \n",
      "57              (prime minister, leader)     (prime)            0.011990   \n",
      "93   (prime minister, blair, chancellor)     (brown)            0.019185   \n",
      "\n",
      "     consequent support   support  confidence       lift  leverage  conviction  \n",
      "107            0.107914  0.014388         1.0   9.266667  0.012836         inf  \n",
      "91             0.091127  0.011990         1.0  10.973684  0.010898         inf  \n",
      "26             0.134293  0.035971         1.0   7.446429  0.031141         inf  \n",
      "57             0.091127  0.011990         1.0  10.973684  0.010898         inf  \n",
      "93             0.134293  0.019185         1.0   7.446429  0.016608         inf  \n",
      "\n",
      ">>> Generate association rules using Lift threshold\n",
      "                   antecedents                 consequents  \\\n",
      "3             (spending, tory)                  (cut, tax)   \n",
      "0                   (cut, tax)            (spending, tory)   \n",
      "9   (blair, prime, chancellor)     (prime minister, brown)   \n",
      "10     (prime minister, brown)  (blair, prime, chancellor)   \n",
      "1                  (cut, tory)             (spending, tax)   \n",
      "\n",
      "    antecedent support  consequent support   support  confidence       lift  \\\n",
      "3             0.016787            0.023981  0.014388    0.857143  35.742857   \n",
      "0             0.023981            0.016787  0.014388    0.600000  35.742857   \n",
      "9             0.016787            0.033573  0.016787    1.000000  29.785714   \n",
      "10            0.033573            0.016787  0.016787    0.500000  29.785714   \n",
      "1             0.023981            0.023981  0.014388    0.600000  25.020000   \n",
      "\n",
      "    leverage  conviction  \n",
      "3   0.013986    6.832134  \n",
      "0   0.013986    2.458034  \n",
      "9   0.016223         inf  \n",
      "10  0.016223    1.966427  \n",
      "1   0.013813    2.440048  \n",
      "\n",
      ">>> Generate association rules using both Lift and Confidence threshold\n",
      "                             antecedents              consequents  \\\n",
      "55                          (tax, local)                (council)   \n",
      "109                      (cut, spending)              (tax, tory)   \n",
      "128           (blair, prime, chancellor)  (prime minister, brown)   \n",
      "125  (prime minister, blair, chancellor)           (brown, prime)   \n",
      "108                (spending, tax, tory)                    (cut)   \n",
      "\n",
      "     antecedent support  consequent support   support  confidence       lift  \\\n",
      "55             0.014388            0.055156  0.014388    1.000000  18.130435   \n",
      "109            0.014388            0.045564  0.014388    1.000000  21.947368   \n",
      "128            0.016787            0.033573  0.016787    1.000000  29.785714   \n",
      "125            0.019185            0.035971  0.016787    0.875000  24.325000   \n",
      "108            0.016787            0.043165  0.014388    0.857143  19.857143   \n",
      "\n",
      "     leverage  conviction  \n",
      "55   0.013595         inf  \n",
      "109  0.013733         inf  \n",
      "128  0.016223         inf  \n",
      "125  0.016096    7.712230  \n",
      "108  0.013664    6.697842  \n"
     ]
    }
   ],
   "source": [
    "politicsdf = pd.DataFrame(dat.loc[dat['category'] == \"politics\"])\n",
    "\n",
    "with open(\"ARMPoliticsKeywords.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(politicsdf['keywords'])\n",
    "\n",
    "ARMPoliticsdata = pd.read_csv('ARMPoliticsKeywords.csv', header = None)\n",
    "\n",
    "politics_list=pd.Series([])\n",
    "for col in ARMPoliticsdata:\n",
    "    politics_list = politics_list.append(ARMPoliticsdata[col].dropna())\n",
    "\n",
    "y = politics_list.value_counts().head(50).to_frame()\n",
    "\n",
    "trans = []\n",
    "for i in range(0, 417):\n",
    "    trans.append([str(ARMPoliticsdata.values[i,j]) for j in range(0, 7)])\n",
    "\n",
    "# conveting it into an numpy array\n",
    "trans = np.array(trans)\n",
    "te = TransactionEncoder()\n",
    "data_encoded = te.fit_transform(trans)\n",
    "data_encoded = pd.DataFrame(data_encoded, columns = te.columns_)\n",
    "\n",
    "data_encoded = data_encoded.loc[:, y.index]\n",
    "\n",
    "frequent_itemsets=apriori(data_encoded, min_support = 0.01, use_colnames = True)\n",
    "rules_c = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.8)\n",
    "print(\"\\n>>> Generate association rules using Confidence threshold\")\n",
    "rules_c.sort_values('confidence', ascending = False, inplace = True)\n",
    "print(rules_c.head(5))\n",
    "rules_l = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=22)\n",
    "rules_l.sort_values('lift', ascending = False, inplace = True)\n",
    "print(\"\\n>>> Generate association rules using Lift threshold\")\n",
    "print(rules_l.head(5))\n",
    "\n",
    "print(\"\\n>>> Generate association rules using both Lift and Confidence threshold\")\n",
    "print(rules_c[ (rules_c['lift'] >= 15) &\n",
    "       (rules_c['confidence'] >= 0.5) ].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_counts.toarray()\n",
    "y = dat['category'].map( {'tech': 0, 'business': 1, 'sport': 2, 'entertainment': 3, 'politics': 4} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split both Inputs (X) and Ouput (y) into training set (70%) and testing set (30%) \n",
    "# Set the random state as 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = text_tfidf.toarray()\n",
    "y = dat['category'].map( {'tech': 0, 'business': 1, 'sport': 2, 'entertainment': 3, 'politics': 4} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split both Inputs (X) and Ouput (y) into training set (70%) and testing set (30%)\n",
    "# Set the random state as 2\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build the Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.940\n",
      "Best cross-validation score: 0.941\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(solver=\"lbfgs\", max_iter=1000), X_train, y_train, cv=15)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver=\"lbfgs\", max_iter=1000), param_grid, cv=15)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.946\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.944\n",
      "Best cross-validation score: 0.944\n",
      "Best parameters:  {'C': 10}\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(solver='lbfgs', max_iter=1000), X2_train, y2_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(solver=\"lbfgs\", max_iter=1000), param_grid, cv=5)\n",
    "grid.fit(X2_train, y2_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.948\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.3f}\".format(grid.score(X2_test, y2_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.938\n",
      "Best cross-validation score: 0.933\n",
      "Best parameters:  {'max_depth': 30, 'n_estimators': 30}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_samples=1000)\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "param = {'n_estimators': [10, 20, 30],\n",
    "        'max_depth': [30, None]}\n",
    "grid = GridSearchCV(rf, param, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.934\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.934\n",
      "Best cross-validation score: 0.930\n",
      "Best parameters:  {'max_depth': None, 'n_estimators': 30}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_samples=900)\n",
    "scores = cross_val_score(rf, X2_train, y2_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "param = {'n_estimators': [10, 20, 30],\n",
    "        'max_depth': [None, 30]}\n",
    "grid = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "grid.fit(X2_train, y2_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.924\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.3f}\".format(grid.score(X2_test, y2_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.945\n",
      "Best cross-validation score: 0.945\n",
      "Best parameters:  {'alpha': 10}\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "param = {'alpha': [1, 2, 5, 10]}\n",
    "grid = GridSearchCV(clf, param, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.942\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.944\n",
      "Best cross-validation score: 0.944\n",
      "Best parameters:  {'alpha': 1}\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, X2_train, y2_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "param = {'alpha': [1, 2, 5, 10]}\n",
    "grid = GridSearchCV(clf, param, cv=5)\n",
    "grid.fit(X2_train, y2_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.943\n"
     ]
    }
   ],
   "source": [
    "print(\"Test score: {:.3f}\".format(grid.score(X2_test, y2_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate and Improve the Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.946\n",
      "Best cross-validation score: 0.947\n",
      "Best parameters:  {'C': 10}\n",
      "Test score: 0.953\n"
     ]
    }
   ],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.35, random_state=2)\n",
    "\n",
    "scores = cross_val_score(LogisticRegression(solver='lbfgs', max_iter=1000), X2_train, y2_train, cv=20)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "param_grid = {'C': [1, 10, 30, 50]}\n",
    "grid = HalvingGridSearchCV(LogisticRegression(solver=\"lbfgs\", max_iter=1000), param_grid, random_state=2, cv=20).fit(X2_train, y2_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n",
    "print(\"Test score: {:.3f}\".format(grid.score(X2_test, y2_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.934\n",
      "Best cross-validation score: 0.940\n",
      "Best parameters:  {'max_depth': None, 'n_estimators': 300}\n",
      "Test score: 0.947\n"
     ]
    }
   ],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.35, random_state=2)\n",
    "\n",
    "rf = RandomForestClassifier(max_samples=1000)\n",
    "scores = cross_val_score(rf, X2_train, y2_train, cv=25)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "param = {'n_estimators': [100, 300, 700],\n",
    "        'max_depth': [5, 30, 70, None]}\n",
    "grid = GridSearchCV(rf, param, cv=25, n_jobs=-1)\n",
    "grid.fit(X2_train, y2_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n",
    "print(\"Test score: {:.3f}\".format(grid.score(X2_test, y2_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.947\n",
      "Best cross-validation score: 0.948\n",
      "Best parameters:  {'alpha': 0.1}\n",
      "Test score: 0.953\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=4)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=20)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "param = {'alpha': [0.1, 1, 2, 5, 10]}\n",
    "grid = GridSearchCV(clf, param, cv=20)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n",
    "print(\"Test score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Final improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.945\n",
      "Best cross-validation score: 0.947\n",
      "Best parameters:  {'alpha': 0.1}\n",
      "Test score: 0.942\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=100)\n",
    "print(\"Mean cross-validation accuracy: {:.3f}\".format(np.mean(scores)))\n",
    "param = {'alpha': [0.1, 1, 10, 50]}\n",
    "grid = HalvingGridSearchCV(clf, param, cv=100).fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n",
    "print(\"Test score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
